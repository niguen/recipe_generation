{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "UNITS = 256\n",
    "\n",
    "BUFFER_SIZE = 120\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "max_name_features = 10000\n",
    "max_name_len = 4\n",
    "\n",
    "max_ingredient_features = 4000\n",
    "max_ingredient_len = 4\n",
    "\n",
    "max_step_features = 10000\n",
    "max_step_len = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = os.path.join('data', 'choc_recipes.txt')\n",
    "# Using readlines()\n",
    "file1 = open(text_path, 'r')\n",
    "Lines = file1.readlines()\n",
    "\n",
    "names = []\n",
    "ingredients = []\n",
    "steps = []\n",
    "\n",
    "count = 0\n",
    "# Strips the newline character\n",
    "for line in Lines:\n",
    "    pieces = line.split('\\t')\n",
    "    names.append(pieces[0])\n",
    "    ingredients.append(pieces[1])\n",
    "    steps.append(pieces[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients_list_list = []\n",
    "for i_string in ingredients:\n",
    "    i = i_string.replace(', ', '*')\n",
    "    i = i.replace(' ', '_')\n",
    "    i = i.replace('*', ', ')\n",
    "\n",
    "    ingredients_list_list.append(i)\n",
    "\n",
    "ingredients = ingredients_list_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name count: 17258\n",
      "Ingredient count: 17258\n",
      "Step count: 17258\n"
     ]
    }
   ],
   "source": [
    "print(f'Name count: {len(names)}')\n",
    "print(f'Ingredient count: {len(ingredients)}')\n",
    "print(f'Step count: {len(steps)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = np.array(names)\n",
    "ingredients = np.array(ingredients)\n",
    "steps = np.array(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(names)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "is_train = np.random.uniform(size=(len(names),)) < 0.8\n",
    "\n",
    "# train\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({\"names\": names[is_train], \"ingredients\": ingredients[is_train]}, steps[is_train]))\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# test\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(({\"names\": names[~is_train], \"ingredients\": ingredients[~is_train]}, steps[~is_train]))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Text Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_vectorizer = tf.keras.layers.TextVectorization(\n",
    " max_tokens=max_step_features,\n",
    " output_mode='int',\n",
    " output_sequence_length=max_step_len)\n",
    "\n",
    "#train_raw.map(lambda context, target: context)\n",
    "step_vectorizer.adapt(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'step', 'and', 'the', 'in', 'a', 'to', 'until', 'with']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_vectorizer.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient_vectorizer = tf.keras.layers.TextVectorization(\n",
    " max_tokens=max_ingredient_features,\n",
    " output_mode='int',\n",
    " output_sequence_length=max_ingredient_len)\n",
    "\n",
    "#train_raw.map(lambda context, target: context)\n",
    "\n",
    "ingredient_vectorizer.adapt(ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'sugar',\n",
       " 'butter',\n",
       " 'salt',\n",
       " 'eggs',\n",
       " 'flour',\n",
       " 'vanilla',\n",
       " 'bakingpowder',\n",
       " 'milk']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredient_vectorizer.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_vectorizer = tf.keras.layers.TextVectorization(\n",
    " max_tokens=max_name_features,\n",
    " output_mode='int',\n",
    " output_sequence_length=max_name_len)\n",
    "\n",
    "#train_raw.map(lambda context, target: context)\n",
    "name_vectorizer.adapt(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'cake',\n",
       " 'cookies',\n",
       " 'chocolate',\n",
       " 's',\n",
       " 'pie',\n",
       " 'cream',\n",
       " 'apple',\n",
       " 'with']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_vectorizer.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(inputs, outputs):\n",
    "\n",
    "  names = inputs['names']\n",
    "  ingredients = inputs['ingredients']\n",
    "\n",
    "  name_context = name_vectorizer(names)\n",
    "  ingredient_context = ingredient_vectorizer(ingredients)\n",
    "  context = {'names': name_context, 'ingredients': ingredient_context}\n",
    "\n",
    "  target = step_vectorizer(outputs)\n",
    "  targ_in = target[:,:-1]\n",
    "  targ_out = target[:,1:]\n",
    "  return (context, targ_in), targ_out\n",
    "\n",
    "\n",
    "train_ds = train_dataset.map(process_text, tf.data.AUTOTUNE)\n",
    "test_ds = test_dataset.map(process_text, tf.data.AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49msaved_model\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mtranslator\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\HRW\\NLP\\nlp-recipe-generation\\venv\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:828\u001b[0m, in \u001b[0;36mload\u001b[1;34m(export_dir, tags, options)\u001b[0m\n\u001b[0;32m    826\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(export_dir, os\u001b[39m.\u001b[39mPathLike):\n\u001b[0;32m    827\u001b[0m   export_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(export_dir)\n\u001b[1;32m--> 828\u001b[0m result \u001b[39m=\u001b[39m load_partial(export_dir, \u001b[39mNone\u001b[39;49;00m, tags, options)[\u001b[39m\"\u001b[39m\u001b[39mroot\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    829\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\HRW\\NLP\\nlp-recipe-generation\\venv\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:958\u001b[0m, in \u001b[0;36mload_partial\u001b[1;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39minit_scope():\n\u001b[0;32m    957\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 958\u001b[0m     loader \u001b[39m=\u001b[39m Loader(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0;32m    959\u001b[0m                     ckpt_options, options, filters)\n\u001b[0;32m    960\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mNotFoundError \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    961\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m    962\u001b[0m         \u001b[39mstr\u001b[39m(err) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m You may be trying to load on a different device \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    963\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfrom the computational device. Consider setting the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    964\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`experimental_io_device` option in `tf.saved_model.LoadOptions` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    965\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mto the io_device such as \u001b[39m\u001b[39m'\u001b[39m\u001b[39m/job:localhost\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\HRW\\NLP\\nlp-recipe-generation\\venv\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:154\u001b[0m, in \u001b[0;36mLoader.__init__\u001b[1;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, save_options, filters)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_proto \u001b[39m=\u001b[39m object_graph_proto\n\u001b[0;32m    152\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_export_dir \u001b[39m=\u001b[39m export_dir\n\u001b[0;32m    153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_functions \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 154\u001b[0m     function_deserialization\u001b[39m.\u001b[39;49mload_function_def_library(\n\u001b[0;32m    155\u001b[0m         library\u001b[39m=\u001b[39;49mmeta_graph\u001b[39m.\u001b[39;49mgraph_def\u001b[39m.\u001b[39;49mlibrary,\n\u001b[0;32m    156\u001b[0m         saved_object_graph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_proto,\n\u001b[0;32m    157\u001b[0m         wrapper_function\u001b[39m=\u001b[39;49m_WrapperFunction))\n\u001b[0;32m    158\u001b[0m \u001b[39m# Store a set of all concrete functions that have been set up with\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[39m# captures.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restored_concrete_functions \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\HRW\\NLP\\nlp-recipe-generation\\venv\\lib\\site-packages\\tensorflow\\python\\saved_model\\function_deserialization.py:416\u001b[0m, in \u001b[0;36mload_function_def_library\u001b[1;34m(library, saved_object_graph, load_shared_name_suffix, wrapper_function)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[39m# There is no need to copy all functions into the function def graph. It\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \u001b[39m# leads to a O(n^2) increase of memory when importing functions and the\u001b[39;00m\n\u001b[0;32m    412\u001b[0m \u001b[39m# extra function definitions are a no-op since they already imported as a\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[39m# function before and passed in explicitly (due to the topologic sort\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[39m# import).\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \u001b[39mwith\u001b[39;00m graph\u001b[39m.\u001b[39mas_default():\n\u001b[1;32m--> 416\u001b[0m   func_graph \u001b[39m=\u001b[39m function_def_lib\u001b[39m.\u001b[39;49mfunction_def_to_graph(\n\u001b[0;32m    417\u001b[0m       fdef,\n\u001b[0;32m    418\u001b[0m       structured_input_signature\u001b[39m=\u001b[39;49mstructured_input_signature,\n\u001b[0;32m    419\u001b[0m       structured_outputs\u001b[39m=\u001b[39;49mstructured_outputs)\n\u001b[0;32m    420\u001b[0m \u001b[39m# Restores gradients for function-call ops (not the same as ops that use\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[39m# custom gradients)\u001b[39;00m\n\u001b[0;32m    422\u001b[0m _restore_gradient_functions(func_graph, renamed_functions, loaded_gradients)\n",
      "File \u001b[1;32mc:\\HRW\\NLP\\nlp-recipe-generation\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\function_def_to_graph.py:87\u001b[0m, in \u001b[0;36mfunction_def_to_graph\u001b[1;34m(fdef, structured_input_signature, structured_outputs, input_shapes)\u001b[0m\n\u001b[0;32m     82\u001b[0m graph_def, nested_to_flat_tensor_name \u001b[39m=\u001b[39m function_def_to_graph_def(\n\u001b[0;32m     83\u001b[0m     fdef, input_shapes)\n\u001b[0;32m     85\u001b[0m \u001b[39mwith\u001b[39;00m func_graph\u001b[39m.\u001b[39mas_default():\n\u001b[0;32m     86\u001b[0m   \u001b[39m# Add all function nodes to the graph.\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m   importer\u001b[39m.\u001b[39;49mimport_graph_def_for_function(graph_def, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     89\u001b[0m   \u001b[39m# Initialize fields specific to FuncGraph.\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \n\u001b[0;32m     91\u001b[0m   \u001b[39m# inputs\u001b[39;00m\n\u001b[0;32m     92\u001b[0m   input_tensor_names \u001b[39m=\u001b[39m [\n\u001b[0;32m     93\u001b[0m       nested_to_flat_tensor_name[arg\u001b[39m.\u001b[39mname] \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m fdef\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39minput_arg\n\u001b[0;32m     94\u001b[0m   ]\n",
      "File \u001b[1;32mc:\\HRW\\NLP\\nlp-recipe-generation\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py:414\u001b[0m, in \u001b[0;36mimport_graph_def_for_function\u001b[1;34m(graph_def, name)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimport_graph_def_for_function\u001b[39m(  \u001b[39m# pylint: disable=invalid-name\u001b[39;00m\n\u001b[0;32m    412\u001b[0m     graph_def, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    413\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Like import_graph_def but does not validate colocation constraints.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 414\u001b[0m   \u001b[39mreturn\u001b[39;00m _import_graph_def_internal(\n\u001b[0;32m    415\u001b[0m       graph_def, validate_colocation_constraints\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\HRW\\NLP\\nlp-recipe-generation\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py:517\u001b[0m, in \u001b[0;36m_import_graph_def_internal\u001b[1;34m(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list)\u001b[0m\n\u001b[0;32m    505\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mstr\u001b[39m(e))\n\u001b[0;32m    507\u001b[0m   \u001b[39m# Create _DefinedFunctions for any imported functions.\u001b[39;00m\n\u001b[0;32m    508\u001b[0m   \u001b[39m#\u001b[39;00m\n\u001b[0;32m    509\u001b[0m   \u001b[39m# We do this by creating _DefinedFunctions directly from `graph_def`, and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    514\u001b[0m   \u001b[39m# TODO(skyewm): fetch the TF_Functions directly from the TF_Graph\u001b[39;00m\n\u001b[0;32m    515\u001b[0m   \u001b[39m# TODO(skyewm): avoid sending serialized FunctionDefs back to the TF_Graph\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m   _ProcessNewOps(graph)\n\u001b[0;32m    519\u001b[0m \u001b[39mif\u001b[39;00m graph_def\u001b[39m.\u001b[39mlibrary \u001b[39mand\u001b[39;00m graph_def\u001b[39m.\u001b[39mlibrary\u001b[39m.\u001b[39mfunction:\n\u001b[0;32m    520\u001b[0m   functions \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfrom_library(graph_def\u001b[39m.\u001b[39mlibrary)\n",
      "File \u001b[1;32mc:\\HRW\\NLP\\nlp-recipe-generation\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py:247\u001b[0m, in \u001b[0;36m_ProcessNewOps\u001b[1;34m(graph)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[39m# Maps from a node to the names of the ops it's colocated with, if colocation\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[39m# is specified in the attributes.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m colocation_pairs \u001b[39m=\u001b[39m {}\n\u001b[1;32m--> 247\u001b[0m \u001b[39mfor\u001b[39;00m new_op \u001b[39min\u001b[39;00m graph\u001b[39m.\u001b[39;49m_add_new_tf_operations(compute_devices\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m):  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    248\u001b[0m   original_device \u001b[39m=\u001b[39m new_op\u001b[39m.\u001b[39mdevice\n\u001b[0;32m    249\u001b[0m   new_op\u001b[39m.\u001b[39m_set_device(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\HRW\\NLP\\nlp-recipe-generation\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3948\u001b[0m, in \u001b[0;36mGraph._add_new_tf_operations\u001b[1;34m(self, compute_devices)\u001b[0m\n\u001b[0;32m   3944\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_not_finalized()\n\u001b[0;32m   3946\u001b[0m \u001b[39m# Create all Operation objects before accessing their inputs since an op may\u001b[39;00m\n\u001b[0;32m   3947\u001b[0m \u001b[39m# be created before its inputs.\u001b[39;00m\n\u001b[1;32m-> 3948\u001b[0m new_ops \u001b[39m=\u001b[39m [\n\u001b[0;32m   3949\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_op_from_tf_operation(c_op, compute_device\u001b[39m=\u001b[39mcompute_devices)\n\u001b[0;32m   3950\u001b[0m     \u001b[39mfor\u001b[39;00m c_op \u001b[39min\u001b[39;00m c_api_util\u001b[39m.\u001b[39mnew_tf_operations(\u001b[39mself\u001b[39m)\n\u001b[0;32m   3951\u001b[0m ]\n\u001b[0;32m   3953\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   3954\u001b[0m \u001b[39mfor\u001b[39;00m op \u001b[39min\u001b[39;00m new_ops:\n",
      "File \u001b[1;32mc:\\HRW\\NLP\\nlp-recipe-generation\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3949\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3944\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_not_finalized()\n\u001b[0;32m   3946\u001b[0m \u001b[39m# Create all Operation objects before accessing their inputs since an op may\u001b[39;00m\n\u001b[0;32m   3947\u001b[0m \u001b[39m# be created before its inputs.\u001b[39;00m\n\u001b[0;32m   3948\u001b[0m new_ops \u001b[39m=\u001b[39m [\n\u001b[1;32m-> 3949\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_op_from_tf_operation(c_op, compute_device\u001b[39m=\u001b[39;49mcompute_devices)\n\u001b[0;32m   3950\u001b[0m     \u001b[39mfor\u001b[39;00m c_op \u001b[39min\u001b[39;00m c_api_util\u001b[39m.\u001b[39mnew_tf_operations(\u001b[39mself\u001b[39m)\n\u001b[0;32m   3951\u001b[0m ]\n\u001b[0;32m   3953\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   3954\u001b[0m \u001b[39mfor\u001b[39;00m op \u001b[39min\u001b[39;00m new_ops:\n",
      "File \u001b[1;32mc:\\HRW\\NLP\\nlp-recipe-generation\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3831\u001b[0m, in \u001b[0;36mGraph._create_op_from_tf_operation\u001b[1;34m(self, c_op, compute_device)\u001b[0m\n\u001b[0;32m   3811\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Creates an `Operation` in this graph from the supplied TF_Operation.\u001b[39;00m\n\u001b[0;32m   3812\u001b[0m \n\u001b[0;32m   3813\u001b[0m \u001b[39mThis method is like create_op() except the new Operation is constructed\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3828\u001b[0m \u001b[39m  An `Operation` object.\u001b[39;00m\n\u001b[0;32m   3829\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3830\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_not_finalized()\n\u001b[1;32m-> 3831\u001b[0m ret \u001b[39m=\u001b[39m Operation\u001b[39m.\u001b[39;49m_from_c_op(c_op\u001b[39m=\u001b[39;49mc_op, g\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   3832\u001b[0m \u001b[39m# If a name_scope was created with ret.name but no nodes were created in it,\u001b[39;00m\n\u001b[0;32m   3833\u001b[0m \u001b[39m# the name will still appear in _names_in_use even though the name hasn't\u001b[39;00m\n\u001b[0;32m   3834\u001b[0m \u001b[39m# been used. This is ok, just leave _names_in_use as-is in this case.\u001b[39;00m\n\u001b[0;32m   3835\u001b[0m \u001b[39m# TODO(skyewm): make the C API guarantee no name conflicts.\u001b[39;00m\n\u001b[0;32m   3836\u001b[0m name_key \u001b[39m=\u001b[39m ret\u001b[39m.\u001b[39mname\u001b[39m.\u001b[39mlower()\n",
      "File \u001b[1;32mc:\\HRW\\NLP\\nlp-recipe-generation\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2135\u001b[0m, in \u001b[0;36mOperation._from_c_op\u001b[1;34m(cls, c_op, g)\u001b[0m\n\u001b[0;32m   2120\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create an Operation from a TF_Operation.\u001b[39;00m\n\u001b[0;32m   2121\u001b[0m \n\u001b[0;32m   2122\u001b[0m \u001b[39mFor internal use only: This is useful for creating Operation for ops\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2131\u001b[0m \u001b[39m  an Operation object.\u001b[39;00m\n\u001b[0;32m   2132\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2133\u001b[0m \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m)\n\u001b[1;32m-> 2135\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_from_c_op(c_op\u001b[39m=\u001b[39;49mc_op, g\u001b[39m=\u001b[39;49mg)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   2136\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\HRW\\NLP\\nlp-recipe-generation\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2172\u001b[0m, in \u001b[0;36mOperation._init_from_c_op\u001b[1;34m(self, c_op, g)\u001b[0m\n\u001b[0;32m   2165\u001b[0m \u001b[39m# Gradient function for this op. There are three ways to specify gradient\u001b[39;00m\n\u001b[0;32m   2166\u001b[0m \u001b[39m# function, and first available gradient gets used, in the following order.\u001b[39;00m\n\u001b[0;32m   2167\u001b[0m \u001b[39m# 1. self._gradient_function\u001b[39;00m\n\u001b[0;32m   2168\u001b[0m \u001b[39m# 2. Gradient name registered by \"_gradient_op_type\" attribute.\u001b[39;00m\n\u001b[0;32m   2169\u001b[0m \u001b[39m# 3. Gradient name registered by op.type.\u001b[39;00m\n\u001b[0;32m   2170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_function \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2172\u001b[0m op_def \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39m_get_op_def(pywrap_tf_session\u001b[39m.\u001b[39;49mTF_OperationOpType(c_op))  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   2174\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_stateful \u001b[39m=\u001b[39m op_def\u001b[39m.\u001b[39mis_stateful\n\u001b[0;32m   2176\u001b[0m \u001b[39m# Initialize self._outputs.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = tf.saved_model.load('translator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cake mix cake mix eggs and oil in a large bowl step beat in eggs one at a time beating well after each addition step add flour mixture and beat until combined step pour into prepared pan step bake at 350 degrees for 25 minutes or until cake tests done'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.translate(['sugar, salt, eggs, vanilla'], ['cake'])\n",
    "result[0].numpy().decode()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_UserObject' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      2\u001b[0m     train_ds\u001b[39m.\u001b[39mrepeat(), \n\u001b[0;32m      3\u001b[0m     epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[0;32m      4\u001b[0m     steps_per_epoch \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m,\n\u001b[0;32m      5\u001b[0m     validation_data\u001b[39m=\u001b[39mtest_ds,\n\u001b[0;32m      6\u001b[0m     validation_steps \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m,\n\u001b[0;32m      7\u001b[0m     callbacks\u001b[39m=\u001b[39m[\n\u001b[0;32m      8\u001b[0m         tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(patience\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)])\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_UserObject' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds.repeat(), \n",
    "    epochs=10,\n",
    "    steps_per_epoch = 100,\n",
    "    validation_data=test_ds,\n",
    "    validation_steps = 20,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=3)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b3b175500661b34c9810fb06f07c17ae229594eb81df44f5824523195c835e54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
